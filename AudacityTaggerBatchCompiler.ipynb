{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJqIOush0tANn0U/lDnmIX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ziforge/bird-net-batch-analysis/blob/main/AudacityTaggerBatchCompiler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¦ Bird Audio Analysis Notebook\n",
        "\n",
        "This notebook processes batches of audio recordings into one file, analyzes them with **BirdNET**, and produces both **labels for Audacity** and **detailed visualizations (spectrograms, F0, F1â€“F3, etc.)**.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Workflow Overview\n",
        "\n",
        "1. **Upload & Merge**  \n",
        "   - Upload audio files  \n",
        "   - Merge them end-to-end (no overlaps)  \n",
        "   - Export: merged WAV, Audacity labels, index CSV  \n",
        "\n",
        "2. **BirdNET Analysis**  \n",
        "   - Run BirdNET on each original file  \n",
        "   - Align detections to the merged timeline  \n",
        "   - Export: detections CSV + Audacity labels  \n",
        "\n",
        "3. **Visualize Detections**  \n",
        "   - Generate waveform + spectrogram plots for each detection  \n",
        "   - Extract F0, F1â€“F3 (formant-like peaks), spectral features  \n",
        "   - Export: summary CSV + images  \n",
        "\n",
        "4. **Overview Spectrogram**  \n",
        "   - Produce one full-file spectrogram  \n",
        "   - Overlay BirdNET detections as shaded spans with labels  \n",
        "\n",
        "5. **Audacity**  \n",
        "   - Open the merged WAV  \n",
        "   - Import both label tracks (per-file + BirdNET)  \n",
        "   - Switch to **Spectrogram View** to see detections aligned with sounds  \n",
        "\n",
        "   \n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Notes\n",
        "\n",
        "- Adjust BirdNET settings (`LAT`, `LON`, `DATE`, `MIN_CONF`) for best results.  \n",
        "- `F0_MIN_HZ` / `F0_MAX_HZ` should match the expected pitch range of your species.  \n",
        "- Formant estimates (F1â€“F3) are approximate **spectral resonances**, not true speech formants.  \n",
        "- You can import both label files into Audacity to see **file boundaries** and **detections** at once.  \n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "ðŸ“„ **License**\n",
        "\n",
        "This notebook and associated code are released under the **MIT License**  \n",
        "Â© 2025 George Redpath. See full license text at the bottom of this notebook.\n",
        "---\n",
        "\n",
        "âš ï¸ **Credit:** If you are using this code, please credit **George Redpath**.  "
      ],
      "metadata": {
        "id": "M6OvdOXSRDg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a Flow chart per Cell on what code does what for an analysis work flow"
      ],
      "metadata": {
        "id": "F1QFEid6YHrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ðŸŽ¨ Aesthetic Pipeline Diagram (Graphviz, inline SVG, HTML-safe)\n",
        "# ============================================================\n",
        "\n",
        "!apt-get -qq install graphviz\n",
        "!pip -q install graphviz\n",
        "\n",
        "from graphviz import Digraph\n",
        "from IPython.display import SVG, display\n",
        "\n",
        "# Theme: \"light\" or \"dark\"\n",
        "THEME = \"light\"\n",
        "\n",
        "# ---- palette ----\n",
        "if THEME == \"dark\":\n",
        "    bg         = \"#0b1220\"\n",
        "    node_fill1 = \"#1b2a4a\"\n",
        "    node_fill2 = \"#0f1c34\"\n",
        "    node_edge  = \"#5aa3ff\"\n",
        "    node_text  = \"#e7efff\"\n",
        "    sub_text   = \"#b8c7e6\"\n",
        "    edge_col   = \"#7fb3ff\"\n",
        "    foot_text  = \"#9fb7dd\"\n",
        "else:\n",
        "    bg         = \"#ffffff\"\n",
        "    node_fill1 = \"#eaf2ff\"\n",
        "    node_fill2 = \"#ffffff\"\n",
        "    node_edge  = \"#2f5bd3\"\n",
        "    node_text  = \"#0f1a2b\"\n",
        "    sub_text   = \"#4a5a73\"\n",
        "    edge_col   = \"#6d9eff\"\n",
        "    foot_text  = \"#6b7b94\"\n",
        "\n",
        "# ---- helpers ----\n",
        "def html_escape(s: str) -> str:\n",
        "    # Graphviz HTML-like labels require escaping &, <, >\n",
        "    return s.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
        "\n",
        "def card(graph: Digraph, node_id: str, title: str, subtitle: str):\n",
        "    title = html_escape(title)\n",
        "    subtitle = html_escape(subtitle)\n",
        "    label = f'''<\n",
        "      <TABLE BORDER=\"0\" CELLBORDER=\"0\" CELLSPACING=\"0\">\n",
        "        <TR><TD><FONT POINT-SIZE=\"14\"><B>{title}</B></FONT></TD></TR>\n",
        "        <TR><TD><FONT COLOR=\"{sub_text}\" POINT-SIZE=\"10\">{subtitle}</FONT></TD></TR>\n",
        "      </TABLE>\n",
        "    >'''\n",
        "    graph.node(\n",
        "        node_id,\n",
        "        label=label,\n",
        "        shape=\"box\",\n",
        "        style=\"rounded,filled\",\n",
        "        fillcolor=f\"{node_fill1}:{node_fill2}\",\n",
        "        gradientangle=\"90\",\n",
        "        color=node_edge,\n",
        "        penwidth=\"1.4\",\n",
        "        fontname=\"Helvetica\",\n",
        "        fontsize=\"12\",\n",
        "        fontcolor=node_text,\n",
        "        margin=\"0.18,0.12\"\n",
        "    )\n",
        "\n",
        "# ---- create graph ----\n",
        "dot = Digraph(\"Bird_Audio_Workflow\", format=\"svg\")\n",
        "dot.attr(\n",
        "    rankdir=\"LR\",\n",
        "    splines=\"spline\",\n",
        "    bgcolor=bg,\n",
        "    pad=\"0.25\",\n",
        "    nodesep=\"0.35\",\n",
        "    ranksep=\"0.6\"\n",
        ")\n",
        "dot.attr(\"edge\",\n",
        "    color=edge_col,\n",
        "    penwidth=\"2\",\n",
        "    arrowsize=\"0.8\",\n",
        "    arrowhead=\"normal\"\n",
        ")\n",
        "\n",
        "# ---- nodes ----\n",
        "card(dot, \"A\", \"Upload & Merge\", \"Upload audio -> merge WAV -> labels + index CSV\")\n",
        "card(dot, \"B\", \"BirdNET Analysis\", \"Run on originals -> align detections to merged timeline\")\n",
        "card(dot, \"C\", \"Visualize Detections\", \"Waveforms + Spectrograms Â· F0/F1/F2/F3 Â· summary CSV\")\n",
        "card(dot, \"D\", \"Overview Spectrogram\", \"Full-file spectrogram with detection overlays\")\n",
        "card(dot, \"E\", \"Audacity\", \"Import labels Â· Spectrogram view Â· Navigate detections\")\n",
        "\n",
        "# ---- edges ----\n",
        "dot.edge(\"A\", \"B\")\n",
        "dot.edge(\"B\", \"C\")\n",
        "dot.edge(\"C\", \"D\")\n",
        "dot.edge(\"D\", \"E\")\n",
        "\n",
        "# ---- footer credit (separate rank)\n",
        "with dot.subgraph(name=\"cluster_legend\") as s:\n",
        "    s.attr(color=\"white\")  # invisible box\n",
        "    s.node(\"credit\", label=f'''<\n",
        "      <FONT POINT-SIZE=\"9\" COLOR=\"{foot_text}\">\n",
        "        MIT License Â· please credit George Redpath\n",
        "      </FONT>\n",
        "    >''', shape=\"plain\")\n",
        "\n",
        "# Render inline\n",
        "display(SVG(dot.pipe(format=\"svg\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "EjNY5k6HW6Ha",
        "outputId": "0d157540-ee3d-4330-f0cb-0c9602f05c1b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1516pt\" height=\"145pt\" viewBox=\"0.00 0.00 1515.50 145.00\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(18 127)\">\n<title>Bird_Audio_Workflow</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-18,18 -18,-127 1497.5,-127 1497.5,18 -18,18\"/>\n<g id=\"clust1\" class=\"cluster\">\n<title>cluster_legend</title>\n<polygon fill=\"#ffffff\" stroke=\"white\" points=\"8,-69 8,-101 251,-101 251,-69 8,-69\"/>\n</g>\n<!-- A -->\n<g id=\"node1\" class=\"node\">\n<title>A</title>\n<defs>\n<linearGradient id=\"l_0\" gradientUnits=\"userSpaceOnUse\" x1=\"129.5\" y1=\"0\" x2=\"129.5\" y2=\"-52\">\n<stop offset=\"0\" style=\"stop-color:#eaf2ff;stop-opacity:1.;\"/>\n<stop offset=\"1\" style=\"stop-color:#ffffff;stop-opacity:1.;\"/>\n</linearGradient>\n</defs>\n<path fill=\"url(#l_0)\" stroke=\"#2f5bd3\" stroke-width=\"1.4\" d=\"M244.5,-52C244.5,-52 14.5,-52 14.5,-52 8.5,-52 2.5,-46 2.5,-40 2.5,-40 2.5,-12 2.5,-12 2.5,-6 8.5,0 14.5,0 14.5,0 244.5,0 244.5,0 250.5,0 256.5,-6 256.5,-12 256.5,-12 256.5,-40 256.5,-40 256.5,-46 250.5,-52 244.5,-52\"/>\n<text text-anchor=\"start\" x=\"77.5\" y=\"-30.8\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\" fill=\"#0f1a2b\">Upload &amp; Merge</text>\n<text text-anchor=\"start\" x=\"17.5\" y=\"-14\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#4a5a73\">Upload audio -&gt; merge WAV -&gt; labels + index CSV</text>\n</g>\n<!-- B -->\n<g id=\"node2\" class=\"node\">\n<title>B</title>\n<defs>\n<linearGradient id=\"l_1\" gradientUnits=\"userSpaceOnUse\" x1=\"433.5\" y1=\"0\" x2=\"433.5\" y2=\"-52\">\n<stop offset=\"0\" style=\"stop-color:#eaf2ff;stop-opacity:1.;\"/>\n<stop offset=\"1\" style=\"stop-color:#ffffff;stop-opacity:1.;\"/>\n</linearGradient>\n</defs>\n<path fill=\"url(#l_1)\" stroke=\"#2f5bd3\" stroke-width=\"1.4\" d=\"M555.5,-52C555.5,-52 311.5,-52 311.5,-52 305.5,-52 299.5,-46 299.5,-40 299.5,-40 299.5,-12 299.5,-12 299.5,-6 305.5,0 311.5,0 311.5,0 555.5,0 555.5,0 561.5,0 567.5,-6 567.5,-12 567.5,-12 567.5,-40 567.5,-40 567.5,-46 561.5,-52 555.5,-52\"/>\n<text text-anchor=\"start\" x=\"377\" y=\"-30.8\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\" fill=\"#0f1a2b\">BirdNET Analysis</text>\n<text text-anchor=\"start\" x=\"314.5\" y=\"-14\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#4a5a73\">Run on originals -&gt; align detections to merged timeline</text>\n</g>\n<!-- A&#45;&gt;B -->\n<g id=\"edge1\" class=\"edge\">\n<title>A-&gt;B</title>\n<path fill=\"none\" stroke=\"#6d9eff\" stroke-width=\"2\" d=\"M256.83,-26C268.14,-26 279.65,-26 291.08,-26\"/>\n<polygon fill=\"#6d9eff\" stroke=\"#6d9eff\" stroke-width=\"2\" points=\"291.39,-28.8 299.39,-26 291.39,-23.2 291.39,-28.8\"/>\n</g>\n<!-- C -->\n<g id=\"node3\" class=\"node\">\n<title>C</title>\n<defs>\n<linearGradient id=\"l_2\" gradientUnits=\"userSpaceOnUse\" x1=\"755\" y1=\"0\" x2=\"755\" y2=\"-52\">\n<stop offset=\"0\" style=\"stop-color:#eaf2ff;stop-opacity:1.;\"/>\n<stop offset=\"1\" style=\"stop-color:#ffffff;stop-opacity:1.;\"/>\n</linearGradient>\n</defs>\n<path fill=\"url(#l_2)\" stroke=\"#2f5bd3\" stroke-width=\"1.4\" d=\"M887.5,-52C887.5,-52 622.5,-52 622.5,-52 616.5,-52 610.5,-46 610.5,-40 610.5,-40 610.5,-12 610.5,-12 610.5,-6 616.5,0 622.5,0 622.5,0 887.5,0 887.5,0 893.5,0 899.5,-6 899.5,-12 899.5,-12 899.5,-40 899.5,-40 899.5,-46 893.5,-52 887.5,-52\"/>\n<text text-anchor=\"start\" x=\"690\" y=\"-30.8\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\" fill=\"#0f1a2b\">Visualize Detections</text>\n<text text-anchor=\"start\" x=\"626\" y=\"-14\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#4a5a73\">Waveforms + Spectrograms Â· F0/F1/F2/F3 Â· summary CSV</text>\n</g>\n<!-- B&#45;&gt;C -->\n<g id=\"edge2\" class=\"edge\">\n<title>B-&gt;C</title>\n<path fill=\"none\" stroke=\"#6d9eff\" stroke-width=\"2\" d=\"M567.68,-26C579.16,-26 590.83,-26 602.43,-26\"/>\n<polygon fill=\"#6d9eff\" stroke=\"#6d9eff\" stroke-width=\"2\" points=\"602.46,-28.8 610.46,-26 602.46,-23.2 602.46,-28.8\"/>\n</g>\n<!-- D -->\n<g id=\"node4\" class=\"node\">\n<title>D</title>\n<defs>\n<linearGradient id=\"l_3\" gradientUnits=\"userSpaceOnUse\" x1=\"1054\" y1=\"0\" x2=\"1054\" y2=\"-52\">\n<stop offset=\"0\" style=\"stop-color:#eaf2ff;stop-opacity:1.;\"/>\n<stop offset=\"1\" style=\"stop-color:#ffffff;stop-opacity:1.;\"/>\n</linearGradient>\n</defs>\n<path fill=\"url(#l_3)\" stroke=\"#2f5bd3\" stroke-width=\"1.4\" d=\"M1153.5,-52C1153.5,-52 954.5,-52 954.5,-52 948.5,-52 942.5,-46 942.5,-40 942.5,-40 942.5,-12 942.5,-12 942.5,-6 948.5,0 954.5,0 954.5,0 1153.5,0 1153.5,0 1159.5,0 1165.5,-6 1165.5,-12 1165.5,-12 1165.5,-40 1165.5,-40 1165.5,-46 1159.5,-52 1153.5,-52\"/>\n<text text-anchor=\"start\" x=\"979.5\" y=\"-30.8\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\" fill=\"#0f1a2b\">Overview Spectrogram</text>\n<text text-anchor=\"start\" x=\"958\" y=\"-14\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#4a5a73\">Full-file spectrogram with detection overlays</text>\n</g>\n<!-- C&#45;&gt;D -->\n<g id=\"edge3\" class=\"edge\">\n<title>C-&gt;D</title>\n<path fill=\"none\" stroke=\"#6d9eff\" stroke-width=\"2\" d=\"M899.72,-26C911.27,-26 922.87,-26 934.22,-26\"/>\n<polygon fill=\"#6d9eff\" stroke=\"#6d9eff\" stroke-width=\"2\" points=\"934.44,-28.8 942.44,-26 934.44,-23.2 934.44,-28.8\"/>\n</g>\n<!-- E -->\n<g id=\"node5\" class=\"node\">\n<title>E</title>\n<defs>\n<linearGradient id=\"l_4\" gradientUnits=\"userSpaceOnUse\" x1=\"1344\" y1=\"0\" x2=\"1344\" y2=\"-52\">\n<stop offset=\"0\" style=\"stop-color:#eaf2ff;stop-opacity:1.;\"/>\n<stop offset=\"1\" style=\"stop-color:#ffffff;stop-opacity:1.;\"/>\n</linearGradient>\n</defs>\n<path fill=\"url(#l_4)\" stroke=\"#2f5bd3\" stroke-width=\"1.4\" d=\"M1467.5,-52C1467.5,-52 1220.5,-52 1220.5,-52 1214.5,-52 1208.5,-46 1208.5,-40 1208.5,-40 1208.5,-12 1208.5,-12 1208.5,-6 1214.5,0 1220.5,0 1220.5,0 1467.5,0 1467.5,0 1473.5,0 1479.5,-6 1479.5,-12 1479.5,-12 1479.5,-40 1479.5,-40 1479.5,-46 1473.5,-52 1467.5,-52\"/>\n<text text-anchor=\"start\" x=\"1315.5\" y=\"-30.8\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\" fill=\"#0f1a2b\">Audacity</text>\n<text text-anchor=\"start\" x=\"1224\" y=\"-14\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#4a5a73\">Import labels Â· Spectrogram view Â· Navigate detections</text>\n</g>\n<!-- D&#45;&gt;E -->\n<g id=\"edge4\" class=\"edge\">\n<title>D-&gt;E</title>\n<path fill=\"none\" stroke=\"#6d9eff\" stroke-width=\"2\" d=\"M1165.52,-26C1176.87,-26 1188.5,-26 1200.13,-26\"/>\n<polygon fill=\"#6d9eff\" stroke=\"#6d9eff\" stroke-width=\"2\" points=\"1200.17,-28.8 1208.17,-26 1200.17,-23.2 1200.17,-28.8\"/>\n</g>\n<!-- credit -->\n<g id=\"node6\" class=\"node\">\n<title>credit</title>\n<text text-anchor=\"start\" x=\"16\" y=\"-82.3\" font-family=\"Times,serif\" font-size=\"14.00\"> Â Â Â Â Â </text>\n<text text-anchor=\"start\" x=\"39\" y=\"-82.3\" font-family=\"Times,serif\" font-size=\"9.00\" fill=\"#6b7b94\"> Â Â Â Â Â Â Â MIT License Â· please credit George Redpath Â Â Â Â Â </text>\n<text text-anchor=\"start\" x=\"227\" y=\"-82.3\" font-family=\"Times,serif\" font-size=\"14.00\"> Â Â Â </text>\n</g>\n</g>\n</svg>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "XAUqPqIKMvuF",
        "outputId": "765694d7-4e44-4003-c0fc-4301d02f0bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cli.github.com/packages stable InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "41 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "ðŸ“‚ Please select your audio files (you can select many at once).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-af47ee15-6490-4e2d-9659-323edef6bfb7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-af47ee15-6490-4e2d-9659-323edef6bfb7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4005494241.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ðŸ“‚ Please select your audio files (you can select many at once).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Upload audio files â†’ Merge end-to-end (no overlaps) â†’ Export + Labels for Audacity\n",
        "# =========================\n",
        "\n",
        "!apt -q update && apt -q install -y ffmpeg\n",
        "!pip -q install pydub pandas\n",
        "\n",
        "import os, glob\n",
        "from pydub import AudioSegment\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# ---- STEP 1: Upload files ----\n",
        "upload_dir = \"/content/uploads\"\n",
        "os.makedirs(upload_dir, exist_ok=True)\n",
        "\n",
        "print(\"ðŸ“‚ Please select your audio files (you can select many at once).\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn, data in uploaded.items():\n",
        "    with open(os.path.join(upload_dir, fn), \"wb\") as f:\n",
        "        f.write(data)\n",
        "\n",
        "print(f\"\\nâœ… Saved {len(uploaded)} files to {upload_dir}\")\n",
        "\n",
        "# ---- USER SETTINGS ----\n",
        "AUDIO_DIR = upload_dir             # Folder with uploaded files\n",
        "GLOB_PATTERN = \"*\"                 # Match all uploaded files\n",
        "SORT_BY = \"name\"                   # \"name\" or \"mtime\"\n",
        "GAP_BETWEEN_MS = 0                 # silence inserted between files (ms). 0 = back-to-back.\n",
        "OUTPUT_BASENAME = \"merged_no_overlap\"\n",
        "EXPORT_WAV = True\n",
        "EXPORT_MP3 = False\n",
        "MP3_BITRATE = \"192k\"\n",
        "\n",
        "# Loudness & format controls (optional)\n",
        "NORMALIZE_EACH = True                # normalize each file before concatenation\n",
        "TARGET_DBF_S = -16.0\n",
        "NORMALIZE_FINAL = True               # normalize the final mix\n",
        "FINAL_TARGET_DBF_S = -14.0\n",
        "FORCE_CHANNELS = 2                   # None to keep as-is; or 1 (mono) / 2 (stereo)\n",
        "FORCE_SAMPLE_RATE = 44100            # None to keep as-is; typical: 44100 or 48000\n",
        "\n",
        "# Label formatting (how labels will look inside Audacity)\n",
        "LABEL_PREFIX = \"\"                    # e.g., \"Part \"\n",
        "LABEL_SHOW_DURATION = True           # append duration to the label text\n",
        "MAX_LABEL_BASENAME_CHARS = 60        # truncate long names in label text\n",
        "\n",
        "# ---- Helpers ----\n",
        "def load_segment(path):\n",
        "    seg = AudioSegment.from_file(path)\n",
        "    if NORMALIZE_EACH and seg.dBFS != float(\"-inf\"):\n",
        "        seg = seg.apply_gain(TARGET_DBF_S - seg.dBFS)\n",
        "    if FORCE_CHANNELS in (1, 2):\n",
        "        seg = seg.set_channels(FORCE_CHANNELS)\n",
        "    if FORCE_SAMPLE_RATE:\n",
        "        seg = seg.set_frame_rate(FORCE_SAMPLE_RATE)\n",
        "    return seg\n",
        "\n",
        "def fmt_time_hhmmss_ms(ms):\n",
        "    s = ms / 1000.0\n",
        "    m, sec = divmod(s, 60)\n",
        "    h, m = divmod(m, 60)\n",
        "    return f\"{int(h):02d}:{int(m):02d}:{sec:06.3f}\"\n",
        "\n",
        "def sec(ms):  # Audacity labels need seconds (float)\n",
        "    return ms / 1000.0\n",
        "\n",
        "# ---- Collect files ----\n",
        "files = [p for p in glob.glob(os.path.join(AUDIO_DIR, GLOB_PATTERN)) if os.path.isfile(p)]\n",
        "if not files:\n",
        "    raise SystemExit(f\"No audio files found in {AUDIO_DIR} matching '{GLOB_PATTERN}'\")\n",
        "\n",
        "if SORT_BY == \"mtime\":\n",
        "    files.sort(key=lambda p: os.path.getmtime(p))\n",
        "else:\n",
        "    files.sort(key=lambda p: os.path.basename(p).lower())\n",
        "\n",
        "print(f\"ðŸ”Ž Found {len(files)} files to concatenate (no overlap).\")\n",
        "\n",
        "# ---- Build master end-to-end + capture label times ----\n",
        "master = AudioSegment.silent(duration=0)\n",
        "gap = AudioSegment.silent(duration=GAP_BETWEEN_MS) if GAP_BETWEEN_MS > 0 else None\n",
        "\n",
        "labels = []   # for Audacity\n",
        "index_rows = []  # for CSV\n",
        "\n",
        "cursor_ms = 0\n",
        "for i, path in enumerate(files, 1):\n",
        "    base = os.path.basename(path)\n",
        "    print(f\"[{i}/{len(files)}] {base}\")\n",
        "    seg = load_segment(path)\n",
        "    start_ms = cursor_ms\n",
        "    end_ms = start_ms + len(seg)\n",
        "\n",
        "    master += seg\n",
        "    if gap and i < len(files):\n",
        "        master += gap\n",
        "        cursor_ms = end_ms + GAP_BETWEEN_MS\n",
        "    else:\n",
        "        cursor_ms = end_ms\n",
        "\n",
        "    # Label text\n",
        "    display_name = os.path.splitext(base)[0][:MAX_LABEL_BASENAME_CHARS]\n",
        "    label_text = f\"{LABEL_PREFIX}{i:03d} - {display_name}\"\n",
        "    if LABEL_SHOW_DURATION:\n",
        "        label_text += f\" ({fmt_time_hhmmss_ms(len(seg))})\"\n",
        "\n",
        "    labels.append((sec(start_ms), sec(end_ms), label_text))\n",
        "    index_rows.append({\n",
        "        \"index\": i,\n",
        "        \"filename\": base,\n",
        "        \"start_ms\": start_ms,\n",
        "        \"end_ms\": end_ms,\n",
        "        \"duration_ms\": len(seg),\n",
        "        \"start_hms\": fmt_time_hhmmss_ms(start_ms),\n",
        "        \"end_hms\": fmt_time_hhmmss_ms(end_ms),\n",
        "        \"duration_hms\": fmt_time_hhmmss_ms(len(seg)),\n",
        "    })\n",
        "\n",
        "# ---- Normalize final output if desired ----\n",
        "if NORMALIZE_FINAL and master.dBFS != float(\"-inf\"):\n",
        "    master = master.apply_gain(FINAL_TARGET_DBF_S - master.dBFS)\n",
        "\n",
        "# ---- Export audio ----\n",
        "out_paths = []\n",
        "if EXPORT_WAV:\n",
        "    wav_path = f\"/content/{OUTPUT_BASENAME}.wav\"\n",
        "    master.export(wav_path, format=\"wav\")\n",
        "    out_paths.append(wav_path)\n",
        "if EXPORT_MP3:\n",
        "    mp3_path = f\"/content/{OUTPUT_BASENAME}.mp3\"\n",
        "    master.export(mp3_path, format=\"mp3\", bitrate=MP3_BITRATE)\n",
        "    out_paths.append(mp3_path)\n",
        "\n",
        "# ---- Write Audacity labels ----\n",
        "labels_path = f\"/content/{OUTPUT_BASENAME}_labels_audacity.txt\"\n",
        "with open(labels_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for s, e, text in labels:\n",
        "        f.write(f\"{s:.6f}\\t{e:.6f}\\t{text}\\n\")\n",
        "out_paths.append(labels_path)\n",
        "\n",
        "# ---- Write index CSV ----\n",
        "df = pd.DataFrame(index_rows)\n",
        "csv_path = f\"/content/{OUTPUT_BASENAME}_index.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "out_paths.append(csv_path)\n",
        "\n",
        "print(\"\\nâœ… Done. Files written:\")\n",
        "for p in out_paths:\n",
        "    print(\" -\", p)\n",
        "\n",
        "print(\"\\nðŸ’¡ To use labels in Audacity: File â†’ Import â†’ Labelsâ€¦ â†’ pick the *_labels_audacity.txt file.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell 2: BirdNET analysis (SEPARATE)\n",
        "# - Runs on original files in AUDIO_DIR\n",
        "# - Uses the index CSV from Cell 1 to align detections on the merged timeline\n",
        "# - Produces: *_birdnet_detections.csv and *_birdnet_labels.txt\n",
        "# =========================\n",
        "\n",
        "!pip -q install birdnetlib==0.18.0 tensorflow==2.15.0 pandas\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from birdnetlib import Recording\n",
        "from birdnetlib.analyzer import Analyzer\n",
        "\n",
        "# ---- Must match Cell 1 ----\n",
        "AUDIO_DIR = \"/content/uploads\"\n",
        "OUTPUT_BASENAME = \"merged_no_overlap\"\n",
        "INDEX_CSV = f\"/content/{OUTPUT_BASENAME}_index.csv\"   # created by Cell 1\n",
        "\n",
        "# ---- BirdNET SETTINGS ----\n",
        "MIN_CONF = 0.25\n",
        "\n",
        "# Optional context (helps reduce false positives by filtering to plausible species)\n",
        "LAT = None            # e.g., 59.9139\n",
        "LON = None            # e.g., 10.7522\n",
        "DATE = None           # e.g., datetime(2025, 5, 20)\n",
        "SPECIES_FILTER = None # e.g., [\"Eurasian Blackbird\", \"Eurasian Wren\"]\n",
        "\n",
        "# ---- Load timeline from index CSV ----\n",
        "if not os.path.exists(INDEX_CSV):\n",
        "    raise SystemExit(f\"Index CSV not found: {INDEX_CSV}. Run Cell 1 first.\")\n",
        "\n",
        "df_idx = pd.read_csv(INDEX_CSV)\n",
        "if not {\"index\",\"filename\",\"start_ms\",\"end_ms\"}.issubset(set(df_idx.columns)):\n",
        "    raise SystemExit(\"Index CSV is missing required columns. Re-run Cell 1.\")\n",
        "\n",
        "df_idx = df_idx.sort_values(\"index\").reset_index(drop=True)\n",
        "\n",
        "print(\"ðŸ¦‰ Initializing BirdNET analyzerâ€¦\")\n",
        "analyzer = Analyzer()  # downloads/loads BirdNET model on first use\n",
        "\n",
        "detections_all = []\n",
        "for _, row in df_idx.iterrows():\n",
        "    fname = row[\"filename\"]\n",
        "    start_ms = float(row[\"start_ms\"])\n",
        "    fpath = os.path.join(AUDIO_DIR, fname)\n",
        "\n",
        "    if not os.path.exists(fpath):\n",
        "        print(f\"âš ï¸ Skipping missing file: {fpath}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Analyzing: {fname}\")\n",
        "    rec = Recording(\n",
        "        analyzer,\n",
        "        fpath,\n",
        "        lat=LAT,\n",
        "        lon=LON,\n",
        "        date=DATE,\n",
        "        min_conf=MIN_CONF,\n",
        "        species_list=SPECIES_FILTER\n",
        "    )\n",
        "    rec.analyze()\n",
        "\n",
        "    for d in rec.detections:\n",
        "        # Shift local per-file seconds into the merged global timeline\n",
        "        g_start_s = (start_ms / 1000.0) + float(d[\"start_time\"])\n",
        "        g_end_s   = (start_ms / 1000.0) + float(d[\"end_time\"])\n",
        "        detections_all.append({\n",
        "            \"file_index\": int(row[\"index\"]),\n",
        "            \"filename\": fname,\n",
        "            \"local_start_s\": float(d[\"start_time\"]),\n",
        "            \"local_end_s\": float(d[\"end_time\"]),\n",
        "            \"global_start_s\": g_start_s,\n",
        "            \"global_end_s\": g_end_s,\n",
        "            \"common_name\": d.get(\"common_name\"),\n",
        "            \"scientific_name\": d.get(\"scientific_name\"),\n",
        "            \"confidence\": float(d.get(\"confidence\", 0.0)),\n",
        "            \"label\": d.get(\"label\")\n",
        "        })\n",
        "\n",
        "# ---- Save outputs ----\n",
        "out_paths = []\n",
        "if detections_all:\n",
        "    df_det = pd.DataFrame(detections_all).sort_values(\"global_start_s\")\n",
        "    det_csv = f\"/content/{OUTPUT_BASENAME}_birdnet_detections.csv\"\n",
        "    df_det.to_csv(det_csv, index=False)\n",
        "    out_paths.append(det_csv)\n",
        "\n",
        "    # Audacity labels (regions) for detections\n",
        "    det_labels = f\"/content/{OUTPUT_BASENAME}_birdnet_labels.txt\"\n",
        "    with open(det_labels, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, r in df_det.iterrows():\n",
        "            text = f\"{r['common_name']} ({r['confidence']:.2f})\"\n",
        "            f.write(f\"{r['global_start_s']:.6f}\\t{r['global_end_s']:.6f}\\t{text}\\n\")\n",
        "    out_paths.append(det_labels)\n",
        "else:\n",
        "    print(\"â„¹ï¸ No BirdNET detections above the confidence threshold.\")\n",
        "\n",
        "print(\"\\nâœ… BirdNET outputs:\")\n",
        "for p in out_paths:\n",
        "    print(\" -\", p)\n",
        "\n",
        "print(\"\\nðŸ’¡ In Audacity you can also import *_birdnet_labels.txt via File â†’ Import â†’ Labelsâ€¦\")"
      ],
      "metadata": {
        "id": "DDo7bvsyRBHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Colab Tool: Visualize BirdNET detections with time+frequency views\n",
        "# - Generates per-detection waveform plot (time domain)\n",
        "# - Generates per-detection spectrogram (frequency domain)\n",
        "# - Estimates F0 with librosa.pyin\n",
        "# - Estimates F1/F2/F3 resonances with LPC (speech-style formant method)\n",
        "# - Writes a summary CSV with metrics + file paths to images\n",
        "# =========================================================\n",
        "\n",
        "!pip -q install librosa pandas numpy scipy matplotlib\n",
        "\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import lfilter, hamming\n",
        "\n",
        "# ------------- USER PATHS -------------\n",
        "WAV_PATH = \"/content/merged_no_overlap.wav\"\n",
        "DETECTIONS_CSV = \"/content/merged_no_overlap_birdnet_detections.csv\"\n",
        "OUT_DIR = \"/content/birdnet_vis\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ------------- USER SETTINGS -------------\n",
        "MIN_CONF = 0.25         # only plot detections >= this confidence\n",
        "MAX_DETECTIONS = None   # e.g., 100 to limit; or None for all\n",
        "PAD_BEFORE_S = 0.2      # seconds of context before a detection\n",
        "PAD_AFTER_S = 0.2       # seconds of context after a detection\n",
        "\n",
        "# F0 (pyin) range â€” adjust for your species\n",
        "F0_MIN_HZ = 100.0\n",
        "F0_MAX_HZ = 8000.0\n",
        "\n",
        "# LPC (formant-like resonances) settings\n",
        "# LPC order heuristic: 2 + sr/1000 works for speech; for birds, slightly higher can help.\n",
        "LPC_ORDER = None        # None => auto: max(8, 2 + sr//1000)\n",
        "PREEMPH = 0.97          # pre-emphasis filter for LPC\n",
        "\n",
        "# Spectrogram settings\n",
        "N_FFT = 2048\n",
        "HOP_LENGTH = 512\n",
        "WIN_LENGTH = 2048\n",
        "\n",
        "# ------------- LOAD AUDIO -------------\n",
        "if not os.path.exists(WAV_PATH):\n",
        "    raise SystemExit(f\"Missing WAV: {WAV_PATH}\")\n",
        "\n",
        "y, sr = librosa.load(WAV_PATH, sr=None, mono=True)\n",
        "\n",
        "# Safety on ranges\n",
        "F0_MIN_HZ = max(10.0, min(F0_MIN_HZ, sr/2.0 - 1))\n",
        "F0_MAX_HZ = max(F0_MIN_HZ + 10.0, min(F0_MAX_HZ, sr/2.0 - 1))\n",
        "if LPC_ORDER is None:\n",
        "    LPC_ORDER = max(8, 2 + sr // 1000)\n",
        "\n",
        "# ------------- LOAD DETECTIONS -------------\n",
        "if not os.path.exists(DETECTIONS_CSV):\n",
        "    raise SystemExit(f\"Missing detections CSV: {DETECTIONS_CSV}\")\n",
        "\n",
        "df = pd.read_csv(DETECTIONS_CSV)\n",
        "required = {\"global_start_s\", \"global_end_s\", \"common_name\", \"confidence\"}\n",
        "if not required.issubset(df.columns):\n",
        "    raise SystemExit(f\"Detections CSV must have columns: {required}\")\n",
        "\n",
        "df = df[df[\"confidence\"] >= MIN_CONF].copy()\n",
        "df = df.sort_values(\"global_start_s\").reset_index(drop=True)\n",
        "if MAX_DETECTIONS is not None:\n",
        "    df = df.head(MAX_DETECTIONS).copy()\n",
        "\n",
        "print(f\"Analyzing {len(df)} detections (conf >= {MIN_CONF}). Output -> {OUT_DIR}\")\n",
        "\n",
        "# ------------- HELPERS -------------\n",
        "\n",
        "def segment_bounds(start_s, end_s, pad_before=0.0, pad_after=0.0, sr=44100, total_len=len(y)):\n",
        "    a = int(max(0, (start_s - pad_before) * sr))\n",
        "    b = int(min(total_len, (end_s + pad_after) * sr))\n",
        "    return a, b\n",
        "\n",
        "def estimate_f0(y_seg, sr):\n",
        "    # Use librosa.pyin for F0 track; returns array with NaN for unvoiced frames\n",
        "    try:\n",
        "        f0, voiced_flag, voiced_prob = librosa.pyin(\n",
        "            y_seg,\n",
        "            fmin=F0_MIN_HZ,\n",
        "            fmax=F0_MAX_HZ,\n",
        "            frame_length=WIN_LENGTH,\n",
        "            hop_length=HOP_LENGTH\n",
        "        )\n",
        "        return f0\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def lpc_coeffs(x, order):\n",
        "    # Autocorrelation method via librosa.lpc\n",
        "    # x should be windowed and pre-emphasized\n",
        "    a = librosa.lpc(x, order=order)\n",
        "    return a\n",
        "\n",
        "def estimate_formants(y_seg, sr, order=12, preemph=0.97):\n",
        "    \"\"\"\n",
        "    Returns up to 3 lowest resonance frequencies (F1, F2, F3) in Hz from LPC roots.\n",
        "    Uses a central window of the segment for stability.\n",
        "    \"\"\"\n",
        "    if len(y_seg) < WIN_LENGTH:\n",
        "        x = y_seg.copy()\n",
        "    else:\n",
        "        center = len(y_seg)//2\n",
        "        half = WIN_LENGTH//2\n",
        "        x = y_seg[max(0, center-half): center+half]\n",
        "\n",
        "    # Hamming window and pre-emphasis\n",
        "    if len(x) < 32:\n",
        "        return [np.nan, np.nan, np.nan]\n",
        "    x = lfilter([1, -preemph], 1, x)\n",
        "    x = x * hamming(len(x), sym=False)\n",
        "\n",
        "    try:\n",
        "        a = lpc_coeffs(x, order=order)\n",
        "    except Exception:\n",
        "        return [np.nan, np.nan, np.nan]\n",
        "\n",
        "    # Roots of LPC polynomial\n",
        "    roots = np.roots(a)\n",
        "    roots = [r for r in roots if np.imag(r) >= 0.01]  # keep one half-plane\n",
        "    angs = np.arctan2(np.imag(roots), np.real(roots))\n",
        "    freqs = np.sort(angs * (sr / (2*np.pi)))\n",
        "\n",
        "    # Keep valid, positive, below Nyquist\n",
        "    freqs = freqs[(freqs > 0) & (freqs < sr/2.0)]\n",
        "    if freqs.size == 0:\n",
        "        return [np.nan, np.nan, np.nan]\n",
        "\n",
        "    # Return first 3\n",
        "    out = list(freqs[:3])\n",
        "    while len(out) < 3:\n",
        "        out.append(np.nan)\n",
        "    return out\n",
        "\n",
        "def spectral_peak_hz(y_seg, sr):\n",
        "    # Simple peak in magnitude spectrum (Hann window)\n",
        "    if len(y_seg) < 4:\n",
        "        return np.nan\n",
        "    S = np.abs(np.fft.rfft(y_seg * np.hanning(len(y_seg))))\n",
        "    freqs = np.fft.rfftfreq(len(y_seg), 1.0/sr)\n",
        "    if len(S) == 0:\n",
        "        return np.nan\n",
        "    idx = int(np.argmax(S))\n",
        "    return float(freqs[idx])\n",
        "\n",
        "def plot_waveform(y_seg, sr, title, out_path):\n",
        "    plt.figure(figsize=(12, 3))\n",
        "    t = np.arange(len(y_seg))/sr\n",
        "    plt.plot(t, y_seg)\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "def plot_spectrogram(y_seg, sr, title, out_path, f0_track=None):\n",
        "    # Linear-frequency spectrogram (amplitude->dB)\n",
        "    S = np.abs(librosa.stft(y_seg, n_fft=N_FFT, hop_length=HOP_LENGTH, win_length=WIN_LENGTH))\n",
        "    S_db = librosa.amplitude_to_db(S, ref=np.max)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    librosa.display.specshow(S_db, sr=sr, hop_length=HOP_LENGTH, x_axis=\"time\", y_axis=\"hz\")\n",
        "    plt.colorbar(label=\"dB\")\n",
        "    plt.title(title)\n",
        "\n",
        "    # Optionally overlay f0 as a line (default color)\n",
        "    if f0_track is not None:\n",
        "        times = librosa.times_like(f0_track, sr=sr, hop_length=HOP_LENGTH)\n",
        "        # mask NaNs for plotting\n",
        "        mask = ~np.isnan(f0_track)\n",
        "        if np.any(mask):\n",
        "            plt.plot(times[mask], f0_track[mask])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "# ------------- PROCESS -------------\n",
        "rows = []\n",
        "for i, r in df.iterrows():\n",
        "    g0 = float(r[\"global_start_s\"])\n",
        "    g1 = float(r[\"global_end_s\"])\n",
        "    species = str(r.get(\"common_name\", \"Unknown\"))\n",
        "    conf = float(r.get(\"confidence\", 0.0))\n",
        "\n",
        "    a, b = segment_bounds(g0, g1, PAD_BEFORE_S, PAD_AFTER_S, sr, len(y))\n",
        "    y_seg = y[a:b]\n",
        "    if y_seg.size < 2:\n",
        "        continue\n",
        "\n",
        "    # Estimates\n",
        "    f0_track = estimate_f0(y_seg, sr)\n",
        "    if f0_track is not None:\n",
        "        f0_vals = f0_track[~np.isnan(f0_track)]\n",
        "        f0_mean = float(np.mean(f0_vals)) if f0_vals.size else np.nan\n",
        "        f0_median = float(np.median(f0_vals)) if f0_vals.size else np.nan\n",
        "    else:\n",
        "        f0_mean = np.nan\n",
        "        f0_median = np.nan\n",
        "\n",
        "    # LPC-based \"formant-like\" resonances\n",
        "    F1, F2, F3 = estimate_formants(y_seg, sr, order=LPC_ORDER, preemph=PREEMPH)\n",
        "\n",
        "    # Frequency-domain features\n",
        "    peak_hz = spectral_peak_hz(y_seg, sr)\n",
        "    spec_centroid = float(librosa.feature.spectral_centroid(y=y_seg, sr=sr).mean())\n",
        "\n",
        "    # Save plots (no subplots; each chart has its own figure)\n",
        "    base = f\"det_{i:04d}\"\n",
        "    wave_png = os.path.join(OUT_DIR, f\"{base}_waveform.png\")\n",
        "    spec_png = os.path.join(OUT_DIR, f\"{base}_spectrogram.png\")\n",
        "\n",
        "    wave_title = f\"{species} (conf {conf:.2f})  [{g0:.2f}sâ€“{g1:.2f}s]  Waveform\"\n",
        "    spec_title = f\"{species} (conf {conf:.2f})  [{g0:.2f}sâ€“{g1:.2f}s]  Spectrogram\"\n",
        "\n",
        "    plot_waveform(y_seg, sr, wave_title, wave_png)\n",
        "    plot_spectrogram(y_seg, sr, spec_title, spec_png, f0_track=f0_track)\n",
        "\n",
        "    rows.append({\n",
        "        \"idx\": i,\n",
        "        \"species\": species,\n",
        "        \"confidence\": conf,\n",
        "        \"global_start_s\": g0,\n",
        "        \"global_end_s\": g1,\n",
        "        \"duration_s\": g1 - g0,\n",
        "        \"f0_mean_hz\": f0_mean,\n",
        "        \"f0_median_hz\": f0_median,\n",
        "        \"F1_hz\": float(F1) if not np.isnan(F1) else np.nan,\n",
        "        \"F2_hz\": float(F2) if not np.isnan(F2) else np.nan,\n",
        "        \"F3_hz\": float(F3) if not np.isnan(F3) else np.nan,\n",
        "        \"spectral_centroid_hz\": spec_centroid,\n",
        "        \"peak_freq_hz\": peak_hz,\n",
        "        \"waveform_png\": wave_png,\n",
        "        \"spectrogram_png\": spec_png\n",
        "    })\n",
        "\n",
        "# ------------- SUMMARY CSV -------------\n",
        "summary_csv = os.path.join(OUT_DIR, \"birdnet_detection_summary.csv\")\n",
        "pd.DataFrame(rows).to_csv(summary_csv, index=False)\n",
        "\n",
        "print(f\"\\nâœ… Done. Wrote {len(rows)} detection visualizations to: {OUT_DIR}\")\n",
        "print(f\"Summary CSV: {summary_csv}\")\n",
        "print(\"Each detection has two images: *_waveform.png and *_spectrogram.png\")"
      ],
      "metadata": {
        "id": "6ZTU8qeaSRaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Overview Spectrogram (entire file) + BirdNET overlays\n",
        "# - One figure (no subplots)\n",
        "# - Uses default matplotlib styling (no custom colors)\n",
        "# ============================================\n",
        "\n",
        "!pip -q install librosa pandas matplotlib\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa, librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- Paths (must match your earlier cells) ----\n",
        "WAV_PATH = \"/content/merged_no_overlap.wav\"\n",
        "DETECTIONS_CSV = \"/content/merged_no_overlap_birdnet_detections.csv\"\n",
        "OUT_DIR = \"/content/birdnet_vis\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---- Display settings ----\n",
        "MIN_CONF = 0.25            # only show detections >= this confidence\n",
        "FMAX_DISPLAY_HZ = None     # e.g., 10000 to crop the y-axis; None shows full range\n",
        "N_FFT = 2048\n",
        "HOP_LENGTH = 512\n",
        "WIN_LENGTH = 2048\n",
        "\n",
        "# ---- Load audio ----\n",
        "if not os.path.exists(WAV_PATH):\n",
        "    raise SystemExit(f\"Missing WAV: {WAV_PATH}\")\n",
        "\n",
        "y, sr = librosa.load(WAV_PATH, sr=None, mono=True)\n",
        "\n",
        "# ---- Compute spectrogram (STFT -> dB) ----\n",
        "S = np.abs(librosa.stft(y, n_fft=N_FFT, hop_length=HOP_LENGTH, win_length=WIN_LENGTH))\n",
        "S_db = librosa.amplitude_to_db(S, ref=np.max)\n",
        "\n",
        "# ---- Load BirdNET detections ----\n",
        "if not os.path.exists(DETECTIONS_CSV):\n",
        "    raise SystemExit(f\"Missing detections CSV: {DETECTIONS_CSV}\")\n",
        "\n",
        "df = pd.read_csv(DETECTIONS_CSV)\n",
        "required = {\"global_start_s\", \"global_end_s\", \"common_name\", \"confidence\"}\n",
        "if not required.issubset(df.columns):\n",
        "    raise SystemExit(f\"Detections CSV must have columns: {required}\")\n",
        "\n",
        "df = df[df[\"confidence\"] >= MIN_CONF].copy()\n",
        "df = df.sort_values(\"global_start_s\").reset_index(drop=True)\n",
        "\n",
        "# ---- Plot (single figure) ----\n",
        "plt.figure(figsize=(14, 6))\n",
        "librosa.display.specshow(\n",
        "    S_db, sr=sr, hop_length=HOP_LENGTH, x_axis=\"time\", y_axis=\"hz\"\n",
        ")\n",
        "plt.colorbar(label=\"dB\")\n",
        "plt.title(\"Overview Spectrogram with BirdNET Detections\")\n",
        "\n",
        "# Optional frequency cap for readability\n",
        "if FMAX_DISPLAY_HZ is not None:\n",
        "    plt.ylim(0, FMAX_DISPLAY_HZ)\n",
        "\n",
        "# Overlay detections: shaded spans + species label\n",
        "ymin, ymax = plt.ylim()\n",
        "for _, r in df.iterrows():\n",
        "    x0, x1 = float(r[\"global_start_s\"]), float(r[\"global_end_s\"])\n",
        "    species = str(r.get(\"common_name\", \"Unknown\"))\n",
        "    conf = float(r.get(\"confidence\", 0.0))\n",
        "\n",
        "    # shaded time region (default color; light transparency)\n",
        "    plt.axvspan(x0, x1, alpha=0.15)\n",
        "\n",
        "    # label text in the top band, centered over the span\n",
        "    xm = (x0 + x1) / 2.0\n",
        "    plt.text(\n",
        "        xm, ymax * 0.96,\n",
        "        f\"{species} ({conf:.2f})\",\n",
        "        ha=\"center\", va=\"top\", rotation=90, fontsize=8\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "out_path = os.path.join(OUT_DIR, \"overview_spectrogram.png\")\n",
        "plt.savefig(out_path, dpi=150)\n",
        "plt.close()\n",
        "\n",
        "print(f\"âœ… Saved: {out_path}\")"
      ],
      "metadata": {
        "id": "SoUXkCygSt26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MIT License\n",
        "\n",
        "Copyright (c) 2025 George Redpath\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ],
      "metadata": {
        "id": "xX32gD3bYaSX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kkhW06HSWcNN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}